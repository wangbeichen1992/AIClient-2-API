import { v4 as uuidv4 } from 'uuid';
import { MODEL_PROTOCOL_PREFIX, getProtocolPrefix } from './common.js';

// =============================================================================
// å¸¸é‡å’Œè¾…åŠ©å‡½æ•°å®šä¹‰
// =============================================================================

// å®šä¹‰é»˜è®¤å¸¸é‡
const DEFAULT_MAX_TOKENS = 8192;
const DEFAULT_GEMINI_MAX_TOKENS = 65536;
const DEFAULT_TEMPERATURE = 1;
const DEFAULT_TOP_P = 0.9;

// è¾…åŠ©å‡½æ•°ï¼šåˆ¤æ–­å€¼æ˜¯å¦ä¸º undefined æˆ– 0ï¼Œå¹¶è¿”å›é»˜è®¤å€¼
function checkAndAssignOrDefault(value, defaultValue) {
    if (value !== undefined && value !== 0) {
        return value;
    }
    return defaultValue;
}

/**
 * æ˜ å°„ç»“æŸåŸå› 
 * @param {string} reason - ç»“æŸåŸå› 
 * @param {string} sourceFormat - æºæ ¼å¼
 * @param {string} targetFormat - ç›®æ ‡æ ¼å¼
 * @returns {string} æ˜ å°„åçš„ç»“æŸåŸå› 
 */
function _mapFinishReason(reason, sourceFormat, targetFormat) {
    const reasonMappings = {
        openai: {
            anthropic: {
                stop: "end_turn",
                length: "max_tokens",
                content_filter: "stop_sequence",
                tool_calls: "tool_use"
            }
        },
        gemini: {
            anthropic: {
                // æ—§ç‰ˆæœ¬å¤§å†™æ ¼å¼
                STOP: "end_turn",
                MAX_TOKENS: "max_tokens",
                SAFETY: "stop_sequence",
                RECITATION: "stop_sequence",
                // æ–°ç‰ˆæœ¬å°å†™æ ¼å¼ï¼ˆv1beta/v1 APIï¼‰
                stop: "end_turn",
                length: "max_tokens",
                safety: "stop_sequence",
                recitation: "stop_sequence",
                other: "end_turn"
            }
        }
    };

    try {
        return reasonMappings[sourceFormat][targetFormat][reason] || "end_turn";
    } catch (e) {
        return "end_turn";
    }
}

/**
 * é€’å½’æ¸…ç†Geminiä¸æ”¯æŒçš„JSON Schemaå±æ€§
 * @param {Object} schema - JSON Schema
 * @returns {Object} æ¸…ç†åçš„JSON Schema
 */
function _cleanJsonSchemaProperties(schema) {
    if (!schema || typeof schema !== 'object') {
        return schema;
    }

    // ç§»é™¤æ‰€æœ‰éæ ‡å‡†å±æ€§
    const sanitized = {};
    for (const [key, value] of Object.entries(schema)) {
        if (["type", "description", "properties", "required", "enum", "items"].includes(key)) {
            sanitized[key] = value;
        }
    }

    if (sanitized.properties && typeof sanitized.properties === 'object') {
        const cleanProperties = {};
        for (const [propName, propSchema] of Object.entries(sanitized.properties)) {
            cleanProperties[propName] = _cleanJsonSchemaProperties(propSchema);
        }
        sanitized.properties = cleanProperties;
    }

    if (sanitized.items) {
        sanitized.items = _cleanJsonSchemaProperties(sanitized.items);
    }

    return sanitized;
}

/**
 * æ ¹æ®budget_tokensæ™ºèƒ½åˆ¤æ–­OpenAI reasoning_effortç­‰çº§
 * @param {number|null} budgetTokens - Anthropic thinkingçš„budget_tokenså€¼
 * @returns {string} OpenAI reasoning_effortç­‰çº§ ("low", "medium", "high")
 */
function _determineReasoningEffortFromBudget(budgetTokens) {
    // å¦‚æœæ²¡æœ‰æä¾›budget_tokensï¼Œé»˜è®¤ä¸ºhigh
    if (budgetTokens === null || budgetTokens === undefined) {
        console.info("No budget_tokens provided, defaulting to reasoning_effort='high'");
        return "high";
    }

    // ä»ç¯å¢ƒå˜é‡è·å–é˜ˆå€¼é…ç½®
    const lowThresholdStr = process.env.ANTHROPIC_TO_OPENAI_LOW_REASONING_THRESHOLD;
    const highThresholdStr = process.env.ANTHROPIC_TO_OPENAI_HIGH_REASONING_THRESHOLD;

    // æ£€æŸ¥å¿…éœ€çš„ç¯å¢ƒå˜é‡
    if (lowThresholdStr === undefined) {
        throw new Error("ANTHROPIC_TO_OPENAI_LOW_REASONING_THRESHOLD environment variable is required for intelligent reasoning_effort determination");
    }

    if (highThresholdStr === undefined) {
        throw new Error("ANTHROPIC_TO_OPENAI_HIGH_REASONING_THRESHOLD environment variable is required for intelligent reasoning_effort determination");
    }

    try {
        const lowThreshold = parseInt(lowThresholdStr, 10);
        const highThreshold = parseInt(highThresholdStr, 10);

        console.debug(`Threshold configuration: low <= ${lowThreshold}, medium <= ${highThreshold}, high > ${highThreshold}`);

        let effort;
        if (budgetTokens <= lowThreshold) {
            effort = "low";
        } else if (budgetTokens <= highThreshold) {
            effort = "medium";
        } else {
            effort = "high";
        }

        console.info(`ğŸ¯ Budget tokens ${budgetTokens} -> reasoning_effort '${effort}' (thresholds: low<=${lowThreshold}, high<=${highThreshold})`);
        return effort;

    } catch (e) {
        throw new Error(`Invalid threshold values in environment variables: ${e.message}. ANTHROPIC_TO_OPENAI_LOW_REASONING_THRESHOLD and ANTHROPIC_TO_OPENAI_HIGH_REASONING_THRESHOLD must be integers.`);
    }
}

// å…¨å±€å·¥å…·çŠ¶æ€ç®¡ç†å™¨
class ToolStateManager {
    constructor() {
        if (ToolStateManager.instance) {
            return ToolStateManager.instance;
        }
        ToolStateManager.instance = this;
        this._toolMappings = {};
        return this;
    }

    // å­˜å‚¨å·¥å…·ååˆ°IDçš„æ˜ å°„
    storeToolMapping(funcName, toolId) {
        this._toolMappings[funcName] = toolId;
    }

    // æ ¹æ®å·¥å…·åè·å–ID
    getToolId(funcName) {
        return this._toolMappings[funcName] || null;
    }

    // æ¸…é™¤æ‰€æœ‰æ˜ å°„
    clearMappings() {
        this._toolMappings = {};
    }
}

// å…¨å±€å·¥å…·çŠ¶æ€ç®¡ç†å™¨å®ä¾‹
const toolStateManager = new ToolStateManager();

// =============================================================================
// ä¸»è½¬æ¢å‡½æ•°
// =============================================================================

/**
 * Generic data conversion function.
 * @param {object} data - The data to convert (request body or response).
 * @param {string} type - The type of conversion: 'request', 'response', 'streamChunk', 'modelList'.
 * @param {string} fromProvider - The source model provider (e.g., MODEL_PROVIDER.GEMINI_CLI).
 * @param {string} toProvider - The target model provider (e.g., MODEL_PROVIDER.OPENAI_CUSTOM).
 * @param {string} [model] - Optional model name for response conversions.
 * @returns {object} The converted data.
 * @throws {Error} If no suitable conversion function is found.
 */
export function convertData(data, type, fromProvider, toProvider, model) {
    // Define a map of conversion functions using protocol prefixes
    const conversionMap = {
        request: {
            [MODEL_PROTOCOL_PREFIX.OPENAI]: { // to OpenAI protocol
                [MODEL_PROTOCOL_PREFIX.GEMINI]: toOpenAIRequestFromGemini, // from Gemini protocol
                [MODEL_PROTOCOL_PREFIX.CLAUDE]: toOpenAIRequestFromClaude, // from Claude protocol
            },
            [MODEL_PROTOCOL_PREFIX.CLAUDE]: { // to Claude protocol
                [MODEL_PROTOCOL_PREFIX.OPENAI]: toClaudeRequestFromOpenAI, // from OpenAI protocol
            },
            [MODEL_PROTOCOL_PREFIX.GEMINI]: { // to Gemini protocol
                [MODEL_PROTOCOL_PREFIX.OPENAI]: toGeminiRequestFromOpenAI, // from OpenAI protocol
                [MODEL_PROTOCOL_PREFIX.CLAUDE]: toGeminiRequestFromClaude, // from Claude protocol
            },
        },
        response: {
            [MODEL_PROTOCOL_PREFIX.OPENAI]: { // to OpenAI protocol
                [MODEL_PROTOCOL_PREFIX.GEMINI]: toOpenAIChatCompletionFromGemini, // from Gemini protocol
                [MODEL_PROTOCOL_PREFIX.CLAUDE]: toOpenAIChatCompletionFromClaude, // from Claude protocol
            },
            [MODEL_PROTOCOL_PREFIX.CLAUDE]: { // to Claude protocol
                [MODEL_PROTOCOL_PREFIX.GEMINI]: toClaudeChatCompletionFromGemini, // from Gemini protocol
                [MODEL_PROTOCOL_PREFIX.OPENAI]: toClaudeChatCompletionFromOpenAI, // from OpenAI protocol
            },
        },
        streamChunk: {
            [MODEL_PROTOCOL_PREFIX.OPENAI]: { // to OpenAI protocol
                [MODEL_PROTOCOL_PREFIX.GEMINI]: toOpenAIStreamChunkFromGemini, // from Gemini protocol
                [MODEL_PROTOCOL_PREFIX.CLAUDE]: toOpenAIStreamChunkFromClaude, // from Claude protocol
            },
            [MODEL_PROTOCOL_PREFIX.CLAUDE]: { // to Claude protocol
                [MODEL_PROTOCOL_PREFIX.GEMINI]: toClaudeStreamChunkFromGemini, // from Gemini protocol
                [MODEL_PROTOCOL_PREFIX.OPENAI]: toClaudeStreamChunkFromOpenAI, // from OpenAI protocol
            },
        },
        modelList: {
            [MODEL_PROTOCOL_PREFIX.OPENAI]: { // to OpenAI protocol
                [MODEL_PROTOCOL_PREFIX.GEMINI]: toOpenAIModelListFromGemini, // from Gemini protocol
                [MODEL_PROTOCOL_PREFIX.CLAUDE]: toOpenAIModelListFromClaude, // from Claude protocol
            },
            [MODEL_PROTOCOL_PREFIX.CLAUDE]: { // to Claude protocol
                [MODEL_PROTOCOL_PREFIX.GEMINI]: toClaudeModelListFromGemini, // from Gemini protocol
                [MODEL_PROTOCOL_PREFIX.OPENAI]: toClaudeModelListFromOpenAI, // from OpenAI protocol
            },
        }
    };

    const targetConversions = conversionMap[type];
    if (!targetConversions) {
        throw new Error(`Unsupported conversion type: ${type}`);
    }
    
    const toConversions = targetConversions[getProtocolPrefix(toProvider)];
    if (!toConversions) {
        throw new Error(`No conversions defined for target protocol: ${getProtocolPrefix(toProvider)} for type: ${type}`);
    }

    const conversionFunction = toConversions[getProtocolPrefix(fromProvider)];
    if (!conversionFunction) {
        throw new Error(`No conversion function found from ${fromProvider} to ${toProvider} for type: ${type}`);
    }
            
    console.log(conversionFunction);
    if (type === 'response' || type === 'streamChunk' || type === 'modelList') {
        return conversionFunction(data, model);
    } else {
        return conversionFunction(data);
    }
}

// =============================================================================
// OpenAI ç›¸å…³è½¬æ¢å‡½æ•°
// =============================================================================

/**
 * Converts a Gemini API request body to an OpenAI chat completion request body.
 * Handles system instructions and role mapping with multimodal support.
 * @param {Object} geminiRequest - The request body from the Gemini API.
 * @returns {Object} The formatted request body for the OpenAI API.
 */
export function toOpenAIRequestFromGemini(geminiRequest) {
    const openaiRequest = {
        messages: [],
        model: geminiRequest.model || "gpt-3.5-turbo", // Default model if not specified in Gemini request
        max_tokens: checkAndAssignOrDefault(geminiRequest.max_tokens, DEFAULT_MAX_TOKENS),
        temperature: checkAndAssignOrDefault(geminiRequest.temperature, DEFAULT_TEMPERATURE),
        top_p: checkAndAssignOrDefault(geminiRequest.top_p, DEFAULT_TOP_P),
    };

    // Process system instruction
    if (geminiRequest.systemInstruction && Array.isArray(geminiRequest.systemInstruction.parts)) {
        const systemContent = processGeminiPartsToOpenAIContent(geminiRequest.systemInstruction.parts);
        if (systemContent) {
            openaiRequest.messages.push({
                role: 'system',
                content: systemContent
            });
        }
    }

    // Process contents
    if (geminiRequest.contents && Array.isArray(geminiRequest.contents)) {
        geminiRequest.contents.forEach(content => {
            if (content && Array.isArray(content.parts)) {
                const openaiContent = processGeminiPartsToOpenAIContent(content.parts);
                if (openaiContent && openaiContent.length > 0) {
                    const openaiRole = content.role === 'model' ? 'assistant' : content.role;
                    openaiRequest.messages.push({
                        role: openaiRole,
                        content: openaiContent
                    });
                }
            }
        });
    }

    return openaiRequest;
}

/**
 * Processes Gemini parts to OpenAI content format with multimodal support.
 * @param {Array} parts - Array of Gemini parts.
 * @returns {Array|string} OpenAI content format.
 */
function processGeminiPartsToOpenAIContent(parts) {
    if (!parts || !Array.isArray(parts)) return '';
    
    const contentArray = [];
    
    parts.forEach(part => {
        if (!part) return;
        
        // Handle text content
        if (typeof part.text === 'string') {
            contentArray.push({
                type: 'text',
                text: part.text
            });
        }
        
        // Handle inline data (images, audio)
        if (part.inlineData) {
            const { mimeType, data } = part.inlineData;
            if (mimeType && data) {
                contentArray.push({
                    type: 'image_url',
                    image_url: {
                        url: `data:${mimeType};base64,${data}`
                    }
                });
            }
        }
        
        // Handle file data
        if (part.fileData) {
            const { mimeType, fileUri } = part.fileData;
            if (mimeType && fileUri) {
                // For file URIs, we need to determine if it's an image or audio
                if (mimeType.startsWith('image/')) {
                    contentArray.push({
                        type: 'image_url',
                        image_url: {
                            url: fileUri
                        }
                    });
                } else if (mimeType.startsWith('audio/')) {
                    // For audio, we'll use a placeholder or handle as text description
                    contentArray.push({
                        type: 'text',
                        text: `[Audio file: ${fileUri}]`
                    });
                }
            }
        }
    });
    
    // Return as array for multimodal, or string for simple text
    return contentArray.length === 1 && contentArray[0].type === 'text'
        ? contentArray[0].text
        : contentArray;
}

export function toOpenAIModelListFromGemini(geminiModels) {
    return {
        object: "list",
        data: geminiModels.models.map(m => ({
            id: m.name.startsWith('models/') ? m.name.substring(7) : m.name, // ç§»é™¤ 'models/' å‰ç¼€ä½œä¸º id
            object: "model",
            created: Math.floor(Date.now() / 1000),
            owned_by: "google",
        })),
    };
}

export function toOpenAIChatCompletionFromGemini(geminiResponse, model) {
    const content = processGeminiResponseContent(geminiResponse);
    
    return {
        id: `chatcmpl-${uuidv4()}`,
        object: "chat.completion",
        created: Math.floor(Date.now() / 1000),
        model: model,
        choices: [{
            index: 0,
            message: {
                role: "assistant",
                content: content
            },
            finish_reason: "stop",
        }],
        usage: geminiResponse.usageMetadata ? {
            prompt_tokens: geminiResponse.usageMetadata.promptTokenCount || 0,
            completion_tokens: geminiResponse.usageMetadata.candidatesTokenCount || 0,
            total_tokens: geminiResponse.usageMetadata.totalTokenCount || 0,
        } : {
            prompt_tokens: 0,
            completion_tokens: 0,
            total_tokens: 0,
        },
    };
}

/**
 * Processes Gemini response content to OpenAI format with multimodal support.
 * @param {Object} geminiResponse - The Gemini API response.
 * @returns {string|Array} Processed content.
 */
function processGeminiResponseContent(geminiResponse) {
    if (!geminiResponse || !geminiResponse.candidates) return '';
    
    const contents = [];
    
    geminiResponse.candidates.forEach(candidate => {
        if (candidate.content && candidate.content.parts) {
            candidate.content.parts.forEach(part => {
                if (part.text) {
                    contents.push(part.text);
                }
                // Note: Gemini response typically doesn't include multimodal content in responses
                // but we handle it for completeness
            });
        }
    });
    
    return contents.join('\n');
}

export function toOpenAIStreamChunkFromGemini(geminiChunk, model) {
    return {
        id: `chatcmpl-${uuidv4()}`, // uuidv4 needs to be imported or handled
        object: "chat.completion.chunk",
        created: Math.floor(Date.now() / 1000),
        model: model,
        choices: [{
            index: 0,
            delta: { content: geminiChunk },
            finish_reason: null,
        }],
        usage: geminiChunk.usageMetadata ? {
            prompt_tokens: geminiChunk.usageMetadata.promptTokenCount || 0,
            completion_tokens: geminiChunk.usageMetadata.candidatesTokenCount || 0,
            total_tokens: geminiChunk.usageMetadata.totalTokenCount || 0,
        } : {
            prompt_tokens: 0,
            completion_tokens: 0,
            total_tokens: 0,
        },
    };
}

/**
 * Converts a Claude API messages response to an OpenAI chat completion response.
 * @param {Object} claudeResponse - The Claude API messages response object.
 * @param {string} model - The model name to include in the response.
 * @returns {Object} The formatted OpenAI chat completion response.
 */
export function toOpenAIChatCompletionFromClaude(claudeResponse, model) {
    if (!claudeResponse || !claudeResponse.content || claudeResponse.content.length === 0) {
        return {
            id: `chatcmpl-${uuidv4()}`,
            object: "chat.completion",
            created: Math.floor(Date.now() / 1000),
            model: model,
            choices: [{
                index: 0,
                message: {
                    role: "assistant",
                    content: "",
                },
                finish_reason: "stop",
            }],
            usage: {
                prompt_tokens: claudeResponse.usage?.input_tokens || 0,
                completion_tokens: claudeResponse.usage?.output_tokens || 0,
                total_tokens: (claudeResponse.usage?.input_tokens || 0) + (claudeResponse.usage?.output_tokens || 0),
            },
        };
    }

    const content = processClaudeResponseContent(claudeResponse.content);
    const finishReason = claudeResponse.stop_reason === 'end_turn' ? 'stop' : claudeResponse.stop_reason;

    return {
        id: `chatcmpl-${uuidv4()}`,
        object: "chat.completion",
        created: Math.floor(Date.now() / 1000),
        model: model,
        choices: [{
            index: 0,
            message: {
                role: "assistant",
                content: content
            },
            finish_reason: finishReason,
        }],
        usage: {
            prompt_tokens: claudeResponse.usage?.input_tokens || 0,
            completion_tokens: claudeResponse.usage?.output_tokens || 0,
            total_tokens: (claudeResponse.usage?.input_tokens || 0) + (claudeResponse.usage?.output_tokens || 0),
        },
    };
}

/**
 * Processes Claude response content to OpenAI format with multimodal support.
 * @param {Array} content - Array of Claude content blocks.
 * @returns {string|Array} Processed content.
 */
function processClaudeResponseContent(content) {
    if (!content || !Array.isArray(content)) return '';
    
    const contentArray = [];
    
    content.forEach(block => {
        if (!block) return;
        
        switch (block.type) {
            case 'text':
                contentArray.push({
                    type: 'text',
                    text: block.text || ''
                });
                break;
                
            case 'image':
                // Handle image blocks from Claude
                if (block.source && block.source.type === 'base64') {
                    contentArray.push({
                        type: 'image_url',
                        image_url: {
                            url: `data:${block.source.media_type};base64,${block.source.data}`
                        }
                    });
                }
                break;
                
            default:
                // Handle other content types as text
                if (block.text) {
                    contentArray.push({
                        type: 'text',
                        text: block.text
                    });
                }
        }
    });
    
    // Return as array for multimodal, or string for simple text
    return contentArray.length === 1 && contentArray[0].type === 'text'
        ? contentArray[0].text
        : contentArray;
}

/**
 * Converts a Claude API messages stream chunk to an OpenAI chat completion stream chunk.
 * Based on the official Claude Messages API stream events.
 * @param {Object} claudeChunk - The Claude API messages stream chunk object.
 * @param {string} [model] - Optional model name to include in the response.
 * @returns {Object} The formatted OpenAI chat completion stream chunk, or an empty object for events that don't map.
 */
export function toOpenAIStreamChunkFromClaude(claudeChunk, model) {
    if (!claudeChunk) {
        return null;
    }
    return {
        id: `chatcmpl-${uuidv4()}`, // uuidv4 needs to be imported or handled
        object: "chat.completion.chunk",
        created: Math.floor(Date.now() / 1000),
        model: model,
        system_fingerprint: "",
        choices: [{
            index: 0,
            delta: { 
                content: claudeChunk,
                reasoning_content: ""
            },
            finish_reason: !claudeChunk ? 'stop' : null,
            message: {
                content: claudeChunk,
                reasoning_content: ""
            }
        }],
        usage:{
            prompt_tokens: 0,
            completion_tokens: 0,
            total_tokens: 0,
        },
    };
}

export function getOpenAIStreamChunkStop(model) {
    return {
        id: `chatcmpl-${uuidv4()}`, // uuidv4 needs to be imported or handled
        object: "chat.completion.chunk",
        created: Math.floor(Date.now() / 1000),
        model: model,
        system_fingerprint: "",
        choices: [{
            index: 0,
            delta: { 
                content: "",
                reasoning_content: ""
            },
            finish_reason: 'stop',
            message: {
                content: "",
                reasoning_content: ""
            }
        }],
        usage:{
            prompt_tokens: 0,
            completion_tokens: 0,
            total_tokens: 0,
        },
    };
}

/**
 * Converts a Claude API model list response to an OpenAI model list response.
 * @param {Array<Object>} claudeModels - The array of model objects from Claude API.
 * @returns {Object} The formatted OpenAI model list response.
 */
export function toOpenAIModelListFromClaude(claudeModels) {
    return {
        object: "list",
        data: claudeModels.models.map(m => ({
            id: m.id || m.name, // Claude models might use 'name' instead of 'id'
            object: "model",
            created: Math.floor(Date.now() / 1000), // Claude may not provide 'created' timestamp
            owned_by: "anthropic",
            // You can add more properties here if they exist in Claude's model response
            // and you want to map them to OpenAI's format, e.g., permissions.
        })),
    };
}

/**
 * Converts an OpenAI chat completion response to a Claude API messages response.
 * @param {Object} openaiResponse - The OpenAI API chat completion response object.
 * @param {string} model - The model name to include in the response.
 * @returns {Object} The formatted Claude API messages response.
 */
export function toClaudeChatCompletionFromOpenAI(openaiResponse, model) {
    if (!openaiResponse || !openaiResponse.choices || openaiResponse.choices.length === 0) {
        return {
            id: `msg_${uuidv4()}`,
            type: "message",
            role: "assistant",
            content: [],
            model: model,
            stop_reason: "end_turn",
            stop_sequence: null,
            usage: {
                input_tokens: openaiResponse?.usage?.prompt_tokens || 0,
                output_tokens: openaiResponse?.usage?.completion_tokens || 0
            }
        };
    }

    const choice = openaiResponse.choices[0];
    const contentList = [];

    // Handle tool calls
    const toolCalls = choice.message?.tool_calls || [];
    for (const toolCall of toolCalls.filter(tc => tc && typeof tc === 'object')) {
        if (toolCall.function) {
            const func = toolCall.function;
            const argStr = func.arguments || "{}";
            let argObj;
            try {
                argObj = typeof argStr === 'string' ? JSON.parse(argStr) : argStr;
            } catch (e) {
                argObj = {};
            }
            contentList.push({
                type: "tool_use",
                id: toolCall.id || "",
                name: func.name || "",
                input: argObj,
            });
        }
    }

    // Handle text content
    const contentText = choice.message?.content || "";
    if (contentText) {
        // ä½¿ç”¨ _extractThinkingFromOpenAIText æå– thinking å†…å®¹
        const extractedContent = _extractThinkingFromOpenAIText(contentText);
        if (Array.isArray(extractedContent)) {
            contentList.push(...extractedContent);
        } else {
            contentList.push({ type: "text", text: extractedContent });
        }
    }

    // Map OpenAI finish reason to Claude stop reason
    const stopReason = _mapFinishReason(
        choice.finish_reason || "stop",
        "openai",
        "anthropic"
    );

    return {
        id: `msg_${uuidv4()}`,
        type: "message",
        role: "assistant",
        content: contentList,
        model: model,
        stop_reason: stopReason,
        stop_sequence: null,
        usage: {
            input_tokens: openaiResponse.usage?.prompt_tokens || 0,
            output_tokens: openaiResponse.usage?.completion_tokens || 0
        }
    };
}

/**
 * Converts a Claude API request body to an OpenAI chat completion request body.
 * Handles system instructions and multimodal content.
 * @param {Object} claudeRequest - The request body from the Claude API.
 * @returns {Object} The formatted request body for the OpenAI API.
 */
export function toOpenAIRequestFromClaude(claudeRequest) {
    const openaiMessages = [];
    let systemMessageContent = '';

    // Add system message if present
    if (claudeRequest.system) {
        systemMessageContent = claudeRequest.system;
    }

    // Process messages
    if (claudeRequest.messages && Array.isArray(claudeRequest.messages)) {
        const tempOpenAIMessages = [];
        for (const msg of claudeRequest.messages) {
            const role = msg.role;

            // å¤„ç†ç”¨æˆ·çš„å·¥å…·ç»“æœæ¶ˆæ¯
            if (role === "user" && Array.isArray(msg.content)) {
                const hasToolResult = msg.content.some(
                    item => item && typeof item === 'object' && item.type === "tool_result"
                );

                if (hasToolResult) {
                    for (const item of msg.content) {
                        if (item && typeof item === 'object' && item.type === "tool_result") {
                            const toolUseId = item.tool_use_id || item.id || "";
                            const contentStr = String(item.content || "");
                            tempOpenAIMessages.push({
                                role: "tool",
                                tool_call_id: toolUseId,
                                content: contentStr,
                            });
                        }
                    }
                    continue; // å·²å¤„ç†å·¥å…·ç»“æœï¼Œè·³è¿‡åç»­å¤„ç†
                }
            }

            // å¤„ç† assistant æ¶ˆæ¯ä¸­çš„å·¥å…·è°ƒç”¨
            if (role === "assistant" && Array.isArray(msg.content) && msg.content.length > 0) {
                const firstPart = msg.content[0];
                if (firstPart.type === "tool_use") {
                    const funcName = firstPart.name || "";
                    const funcArgs = firstPart.input || {};
                    tempOpenAIMessages.push({
                        role: "assistant",
                        content: '',
                        tool_calls: [
                            {
                                id: firstPart.id || `call_${funcName}_1`,
                                type: "function",
                                function: {
                                    name: funcName,
                                    arguments: JSON.stringify(funcArgs)
                                },
                                index: firstPart.index || 0
                            }
                        ]
                    });
                    continue; // å·²å¤„ç†
                }
            }

            // æ™®é€šæ–‡æœ¬æ¶ˆæ¯
            const contentConverted = processClaudeContentToOpenAIContent(msg.content || "");
            // è·³è¿‡ç©ºæ¶ˆæ¯ï¼Œé¿å…åœ¨å†å²ä¸­æ’å…¥ç©ºå­—ç¬¦ä¸²å¯¼è‡´æ¨¡å‹è¯¯åˆ¤
            if (contentConverted && (Array.isArray(contentConverted) ? contentConverted.length > 0 : contentConverted.trim().length > 0)) {
                tempOpenAIMessages.push({
                    role: role,
                    content: contentConverted
                });
            }
        }
        
        // ---------------- OpenAI å…¼å®¹æ€§æ ¡éªŒ ----------------
        // ç¡®ä¿æ‰€æœ‰ assistant.tool_calls å‡æœ‰åç»­ tool å“åº”æ¶ˆæ¯ï¼›å¦åˆ™ç§»é™¤ä¸åŒ¹é…çš„ tool_call
        const validatedMessages = [];
        for (let idx = 0; idx < tempOpenAIMessages.length; idx++) {
            const m = tempOpenAIMessages[idx];
            if (m.role === "assistant" && m.tool_calls) {
                const callIds = m.tool_calls.map(tc => tc.id).filter(id => id);
                // ç»Ÿè®¡åç»­æ˜¯å¦æœ‰å¯¹åº”çš„ tool æ¶ˆæ¯
                let unmatched = new Set(callIds);
                for (let laterIdx = idx + 1; laterIdx < tempOpenAIMessages.length; laterIdx++) {
                    const later = tempOpenAIMessages[laterIdx];
                    if (later.role === "tool" && unmatched.has(later.tool_call_id)) {
                        unmatched.delete(later.tool_call_id);
                    }
                    if (unmatched.size === 0) {
                        break;
                    }
                }
                if (unmatched.size > 0) {
                    // ç§»é™¤æ— åŒ¹é…çš„ tool_call
                    m.tool_calls = m.tool_calls.filter(tc => !unmatched.has(tc.id));
                    // å¦‚æœå…¨éƒ¨è¢«ç§»é™¤ï¼Œåˆ™é™çº§ä¸ºæ™®é€š assistant æ–‡æœ¬æ¶ˆæ¯
                    if (m.tool_calls.length === 0) {
                        delete m.tool_calls;
                        if (m.content === null) {
                            m.content = "";
                        }
                    }
                }
            }
            validatedMessages.push(m);
        }
        openaiMessages.push(...validatedMessages);
    }

    const openaiRequest = {
        model: claudeRequest.model || 'gpt-3.5-turbo', // Default OpenAI model
        messages: openaiMessages,
        max_tokens: checkAndAssignOrDefault(claudeRequest.max_tokens, DEFAULT_MAX_TOKENS),
        temperature: checkAndAssignOrDefault(claudeRequest.temperature, DEFAULT_TEMPERATURE),
        top_p: checkAndAssignOrDefault(claudeRequest.top_p, DEFAULT_TOP_P),
        stream: claudeRequest.stream, // Stream mode is handled by different endpoint
    };

    // Process tools
    if (claudeRequest.tools) {
        const openaiTools = [];
        for (const tool of claudeRequest.tools) {
            openaiTools.push({
                type: "function",
                function: {
                    name: tool.name || "",
                    description: tool.description || "",
                    parameters: _cleanJsonSchemaProperties(tool.input_schema || {}) // ä½¿ç”¨æ¸…ç†å‡½æ•°
                }
            });
        }
        openaiRequest.tools = openaiTools;
        openaiRequest.tool_choice = "auto";
    }

    // å¤„ç†æ€è€ƒé¢„ç®—è½¬æ¢ (Anthropic thinking -> OpenAI reasoning_effort + max_completion_tokens)
    if (claudeRequest.thinking && claudeRequest.thinking.type === "enabled") {
        const budgetTokens = claudeRequest.thinking.budget_tokens;
        // æ ¹æ®budget_tokensæ™ºèƒ½åˆ¤æ–­reasoning_effortç­‰çº§
        const reasoningEffort = _determineReasoningEffortFromBudget(budgetTokens);
        openaiRequest.reasoning_effort = reasoningEffort;

        // å¤„ç†max_completion_tokensçš„ä¼˜å…ˆçº§é€»è¾‘
        let maxCompletionTokens = null;

        // ä¼˜å…ˆçº§1ï¼šå®¢æˆ·ç«¯ä¼ å…¥çš„max_tokens
        if (claudeRequest.max_tokens !== undefined) {
            maxCompletionTokens = claudeRequest.max_tokens;
            delete openaiRequest.max_tokens; // ç§»é™¤max_tokensï¼Œä½¿ç”¨max_completion_tokens
            console.info(`Using client max_tokens as max_completion_tokens: ${maxCompletionTokens}`);
        } else {
            // ä¼˜å…ˆçº§2ï¼šç¯å¢ƒå˜é‡OPENAI_REASONING_MAX_TOKENS
            const envMaxTokens = process.env.OPENAI_REASONING_MAX_TOKENS;
            if (envMaxTokens) {
                try {
                    maxCompletionTokens = parseInt(envMaxTokens, 10);
                    console.info(`Using OPENAI_REASONING_MAX_TOKENS from environment: ${maxCompletionTokens}`);
                } catch (e) {
                    console.warn(`Invalid OPENAI_REASONING_MAX_TOKENS value '${envMaxTokens}', must be integer`);
                }
            }

            if (!envMaxTokens) {
                // ä¼˜å…ˆçº§3ï¼šéƒ½æ²¡æœ‰åˆ™æŠ¥é”™
                throw new Error("For OpenAI reasoning models, max_completion_tokens is required. Please specify max_tokens in the request or set OPENAI_REASONING_MAX_TOKENS environment variable.");
            }
        }
        openaiRequest.max_completion_tokens = maxCompletionTokens;
        console.info(`Anthropic thinking enabled -> OpenAI reasoning_effort='${reasoningEffort}', max_completion_tokens=${maxCompletionTokens}`);
        if (budgetTokens) {
            console.info(`Budget tokens: ${budgetTokens} -> reasoning_effort: '${reasoningEffort}'`);
        }
    }
    
    // Add system message at the beginning if present
    if (systemMessageContent) {
        let stringifiedSystemMessageContent = systemMessageContent;
        if(Array.isArray(systemMessageContent)){
            stringifiedSystemMessageContent = systemMessageContent.map(item =>
                    typeof item === 'string' ? item : item.text).join('\n');
        }
        openaiRequest.messages.unshift({ role: 'system', content: stringifiedSystemMessageContent });
    }

    return openaiRequest;
}

/**
 * Processes Claude content to OpenAI content format with multimodal support.
 * @param {Array} content - Array of Claude content blocks.
 * @returns {Array} OpenAI content format.
 */
function processClaudeContentToOpenAIContent(content) {
    if (!content || !Array.isArray(content)) return [];
    
    const contentArray = [];
    
    content.forEach(block => {
        if (!block) return;
        
        switch (block.type) {
            case 'text':
                if (block.text) {
                    contentArray.push({
                        type: 'text',
                        text: block.text
                    });
                }
                break;
                
            case 'image':
                // Handle image blocks from Claude
                if (block.source && block.source.type === 'base64') {
                    contentArray.push({
                        type: 'image_url',
                        image_url: {
                            url: `data:${block.source.media_type};base64,${block.source.data}`
                        }
                    });
                }
                break;
                
            case 'tool_use':
                // Handle tool use as text
                contentArray.push({
                    type: 'text',
                    text: `[Tool use: ${block.name}]`
                });
                break;
                
            case 'tool_result':
                // Handle tool results as text
                contentArray.push({
                    type: 'text',
                    text: typeof block.content === 'string' ? block.content : JSON.stringify(block.content)
                });
                break;
                
            default:
                // Handle any other content types as text
                if (block.text) {
                    contentArray.push({
                        type: 'text',
                        text: block.text
                    });
                }
        }
    });
    
    return contentArray;
}

// =============================================================================
// Gemini ç›¸å…³è½¬æ¢å‡½æ•°
// =============================================================================

/**
 * Converts an OpenAI chat completion request body to a Gemini API request body.
 * Handles system instructions and merges consecutive messages of the same role with multimodal support.
 * @param {Object} openaiRequest - The request body from the OpenAI API.
 * @returns {Object} The formatted request body for the Gemini API.
 */
export function toGeminiRequestFromOpenAI(openaiRequest) {
    const messages = openaiRequest.messages || [];
    const { systemInstruction, nonSystemMessages } = extractAndProcessSystemMessages(messages);
    
    // Process messages with role conversion and multimodal support
    const processedMessages = [];
    let lastMessage = null;
    
    for (const message of nonSystemMessages) {
        const geminiRole = message.role === 'assistant' ? 'model' : message.role;
        
        // Handle tool responses
        if (geminiRole === 'tool') {
            if (lastMessage) processedMessages.push(lastMessage);
            processedMessages.push({
                role: 'function',
                parts: [{
                    functionResponse: {
                        name: message.name,
                        response: { content: safeParseJSON(message.content) }
                    }
                }]
            });
            lastMessage = null;
            continue;
        }
        
        // Process multimodal content
        const processedContent = processOpenAIContentToGeminiParts(message.content);
        
        // Merge consecutive text messages
        if (lastMessage && lastMessage.role === geminiRole && !message.tool_calls &&
            Array.isArray(processedContent) && processedContent.every(p => p.text) &&
            Array.isArray(lastMessage.parts) && lastMessage.parts.every(p => p.text)) {
            lastMessage.parts.push(...processedContent);
            continue;
        }
        
        if (lastMessage) processedMessages.push(lastMessage);
        lastMessage = { role: geminiRole, parts: processedContent };
    }
    if (lastMessage) processedMessages.push(lastMessage);
    
    // Build Gemini request
    const geminiRequest = {
        contents: processedMessages.filter(item => item.parts && item.parts.length > 0)
    };
    
    if (systemInstruction) geminiRequest.systemInstruction = systemInstruction;
    
    // Handle tools and tool_choice
    if (openaiRequest.tools?.length) {
        geminiRequest.tools = openaiRequest.tools.map(t => {
            const tool = {};
            tool[t.function.name] = t.function.parameters || {};
            return tool;
        });
    }
    
    if (openaiRequest.tool_choice) {
        geminiRequest.toolConfig = buildToolConfig(openaiRequest.tool_choice);
    }
    
    // Add generation config
    const config = buildGenerationConfig(openaiRequest);
    if (Object.keys(config).length) geminiRequest.generationConfig = config;
    
    // Validation
    if (geminiRequest.contents[0]?.role !== 'user') {
        console.warn(`[Request Conversion] Warning: Conversation does not start with a 'user' role.`);
    }
    
    return geminiRequest;
}

/**
 * Processes OpenAI content to Gemini parts format with multimodal support.
 * @param {string|Array} content - OpenAI message content.
 * @returns {Array} Array of Gemini parts.
 */
function processOpenAIContentToGeminiParts(content) {
    if (!content) return [];
    
    // Handle string content
    if (typeof content === 'string') {
        return [{ text: content }];
    }
    
    // Handle array content (multimodal)
    if (Array.isArray(content)) {
        const parts = [];
        
        content.forEach(item => {
            if (!item) return;
            
            switch (item.type) {
                case 'text':
                    if (item.text) {
                        parts.push({ text: item.text });
                    }
                    break;
                    
                case 'image_url':
                    if (item.image_url) {
                        const imageUrl = typeof item.image_url === 'string'
                            ? item.image_url
                            : item.image_url.url;
                            
                        if (imageUrl.startsWith('data:')) {
                            // Handle base64 data URL
                            const [header, data] = imageUrl.split(',');
                            const mimeType = header.match(/data:([^;]+)/)?.[1] || 'image/jpeg';
                            parts.push({
                                inlineData: {
                                    mimeType,
                                    data
                                }
                            });
                        } else {
                            // Handle regular URL
                            parts.push({
                                fileData: {
                                    mimeType: 'image/jpeg', // Default MIME type
                                    fileUri: imageUrl
                                }
                            });
                        }
                    }
                    break;
                    
                case 'audio':
                    // Handle audio content
                    if (item.audio_url) {
                        const audioUrl = typeof item.audio_url === 'string'
                            ? item.audio_url
                            : item.audio_url.url;
                            
                        if (audioUrl.startsWith('data:')) {
                            const [header, data] = audioUrl.split(',');
                            const mimeType = header.match(/data:([^;]+)/)?.[1] || 'audio/wav';
                            parts.push({
                                inlineData: {
                                    mimeType,
                                    data
                                }
                            });
                        } else {
                            parts.push({
                                fileData: {
                                    mimeType: 'audio/wav', // Default MIME type
                                    fileUri: audioUrl
                                }
                            });
                        }
                    }
                    break;
            }
        });
        
        return parts;
    }
    
    return [];
}

function safeParseJSON(str) {
    if (!str) {
        return str;
    }
    let cleanedStr = str;

    // å¤„ç†å¯èƒ½è¢«æˆªæ–­çš„è½¬ä¹‰åºåˆ—
    if (cleanedStr.endsWith('\\') && !cleanedStr.endsWith('\\\\')) {
        cleanedStr = cleanedStr.substring(0, cleanedStr.length - 1); // ç§»é™¤æ‚¬æŒ‚çš„åæ–œæ 
    } else if (cleanedStr.endsWith('\\u') || cleanedStr.endsWith('\\u0') || cleanedStr.endsWith('\\u00')) {
        // ä¸å®Œæ•´çš„Unicodeè½¬ä¹‰åºåˆ—
        const idx = cleanedStr.lastIndexOf('\\u');
        cleanedStr = cleanedStr.substring(0, idx);
    }

    try {
        return JSON.parse(cleanedStr || '{}');
    } catch (e) {
        // å¦‚æœæ¸…ç†åä»ç„¶æ— æ³•è§£æï¼Œåˆ™è¿”å›åŸå§‹å­—ç¬¦ä¸²æˆ–è¿›è¡Œå…¶ä»–é”™è¯¯å¤„ç†
        return str;
    }
}

function buildToolConfig(toolChoice) {
    if (typeof toolChoice === 'string' && ['none', 'auto'].includes(toolChoice)) {
        return { functionCallingConfig: { mode: toolChoice.toUpperCase() } };
    }
    if (typeof toolChoice === 'object' && toolChoice.function) {
        return { functionCallingConfig: { mode: 'ANY', allowedFunctionNames: [toolChoice.function.name] } };
    }
    return null;
}

/**
 * æ ¹æ® tool_result å­—æ®µæ„é€  Gemini functionResponse
 * @param {Object} item - å·¥å…·ç»“æœé¡¹
 * @returns {Object|null} functionResponse å¯¹è±¡
 */
function _buildFunctionResponse(item) {
    if (!item || typeof item !== 'object') {
        return null;
    }

    // åˆ¤å®šæ˜¯å¦ä¸ºå·¥å…·ç»“æœ
    const isResult = (
        item.type === "tool_result" ||
        item.tool_use_id !== undefined ||
        item.tool_output !== undefined ||
        item.result !== undefined ||
        item.content !== undefined
    );
    if (!isResult) {
        return null;
    }

    // æå–å‡½æ•°å
    let funcName = null;

    // æ–¹æ³•1ï¼šä»æ˜ å°„è¡¨ä¸­è·å–ï¼ˆAnthropicæ ¼å¼ï¼‰
    const toolUseId = item.tool_use_id || item.id;
    // è¿™é‡Œéœ€è¦æ³¨æ„ï¼ŒAnthropicConverterå†…éƒ¨ç»´æŠ¤çš„_toolUseMappingæ˜¯ç±»çš„ç§æœ‰å±æ€§ï¼Œåœ¨convert.jsä¸­æ— æ³•ç›´æ¥è®¿é—®
    // å› æ­¤ï¼Œè¿™é‡Œéœ€è¦ä¾èµ–å…¨å±€çš„toolStateManager
    // if (toolUseId && this._toolUseMapping) { // è¿™è¡Œä»£ç åœ¨convert.jsä¸­å°†æ— æ³•ä½¿ç”¨
    //     funcName = this._toolUseMapping[toolUseId];
    // }

    // æ–¹æ³•1.5ï¼šä½¿ç”¨å…¨å±€å·¥å…·çŠ¶æ€ç®¡ç†å™¨
    if (!funcName && toolUseId) {
        // å…ˆå°è¯•ä»IDä¸­æå–å¯èƒ½çš„å‡½æ•°å
        let potentialFuncName = null;
        if (String(toolUseId).startsWith("call_")) {
            const nameAndHash = toolUseId.substring(4); // å»æ‰ "call_" å‰ç¼€
            potentialFuncName = nameAndHash.substring(0, nameAndHash.lastIndexOf("_"));
        }

        // æ£€æŸ¥å…¨å±€ç®¡ç†å™¨ä¸­æ˜¯å¦æœ‰å¯¹åº”çš„æ˜ å°„
        if (potentialFuncName) {
            const storedId = toolStateManager.getToolId(potentialFuncName);
            if (storedId === toolUseId) {
                funcName = potentialFuncName;
            }
        }
    }

    // æ–¹æ³•2ï¼šä» tool_use_id ä¸­æå–ï¼ˆOpenAIæ ¼å¼ï¼‰
    if (!funcName && toolUseId && String(toolUseId).startsWith("call_")) {
        // æ ¼å¼: call_<function_name>_<hash> ï¼Œå‡½æ•°åå¯èƒ½åŒ…å«å¤šä¸ªä¸‹åˆ’çº¿
        const nameAndHash = toolUseId.substring(4); // å»æ‰ "call_" å‰ç¼€
        funcName = nameAndHash.substring(0, nameAndHash.lastIndexOf("_")); // å»æ‰æœ€åä¸€ä¸ª hash æ®µ
    }

    // æ–¹æ³•3ï¼šç›´æ¥ä»å­—æ®µè·å–
    if (!funcName) {
        funcName = (
            item.tool_name ||
            item.name ||
            item.function_name
        );
    }

    if (!funcName) {
        return null;
    }

    // æå–ç»“æœå†…å®¹
    let funcResponse = null;

    // å°è¯•å¤šä¸ªå¯èƒ½çš„ç»“æœå­—æ®µ
    for (const key of ["content", "tool_output", "output", "response", "result"]) {
        if (item[key] !== undefined) {
            funcResponse = item[key];
            break;
        }
    }

    // å¦‚æœ content æ˜¯åˆ—è¡¨ï¼Œå°è¯•æå–æ–‡æœ¬
    if (Array.isArray(funcResponse) && funcResponse.length > 0) {
        const textParts = funcResponse
            .filter(p => p && typeof p === 'object' && p.type === "text")
            .map(p => p.text || "");
        if (textParts.length > 0) {
            funcResponse = textParts.join("");
        }
    }

    // ç¡®ä¿æœ‰å“åº”å†…å®¹
    if (funcResponse === null || funcResponse === undefined) {
        funcResponse = "";
    }

    // Gemini è¦æ±‚ response ä¸º JSON å¯¹è±¡ï¼Œè‹¥ä¸ºåŸå§‹å­—ç¬¦ä¸²åˆ™åŒ…è£…
    if (typeof funcResponse !== 'object') {
        funcResponse = { content: String(funcResponse) };
    }

    return {
        functionResponse: {
            name: funcName,
            response: funcResponse
        }
    };
}

/**
 * Converts a Gemini API model list response to a Claude API model list response.
 * @param {Object} geminiModels - The Gemini API model list response object.
 * @returns {Object} The formatted Claude API model list response.
 */
export function toClaudeModelListFromGemini(geminiModels) {
    return {
        models: geminiModels.models.map(m => ({
            name: m.name.startsWith('models/') ? m.name.substring(7) : m.name, // ç§»é™¤ 'models/' å‰ç¼€ä½œä¸º name
            // Claude models å¯èƒ½åŒ…å«å…¶ä»–å­—æ®µï¼Œè¿™é‡Œä½¿ç”¨é»˜è®¤å€¼
            description: "", // Gemini models ä¸æä¾›æè¿°
            // Claude API å¯èƒ½éœ€è¦å…¶ä»–å­—æ®µï¼Œæ ¹æ®å®é™… API æ–‡æ¡£è°ƒæ•´
        })),
    };
}

/**
 * Converts an OpenAI API model list response to a Claude API model list response.
 * @param {Object} openaiModels - The OpenAI API model list response object.
 * @returns {Object} The formatted Claude API model list response.
 */
export function toClaudeModelListFromOpenAI(openaiModels) {
    return {
        models: openaiModels.data.map(m => ({
            name: m.id, // OpenAI çš„ id æ˜ å°„ä¸º Claude çš„ name
            // Claude models å¯èƒ½åŒ…å«å…¶ä»–å­—æ®µï¼Œè¿™é‡Œä½¿ç”¨é»˜è®¤å€¼
            description: "", // OpenAI models ä¸æä¾›æè¿°
            // Claude API å¯èƒ½éœ€è¦å…¶ä»–å­—æ®µï¼Œæ ¹æ®å®é™… API æ–‡æ¡£è°ƒæ•´
        })),
    };
}

/**
 * ä»OpenAIæ–‡æœ¬ä¸­æå–thinkingå†…å®¹ï¼Œè¿”å›Anthropicæ ¼å¼çš„content blocks
 * @param {string} text - æ–‡æœ¬å†…å®¹
 * @returns {string|Array} æå–åçš„å†…å®¹
 */
function _extractThinkingFromOpenAIText(text) {
    // åŒ¹é… <thinking>...</thinking> æ ‡ç­¾
    const thinkingPattern = /<thinking>\s*(.*?)\s*<\/thinking>/gs;
    const matches = [...text.matchAll(thinkingPattern)];

    const contentBlocks = [];
    let lastEnd = 0;

    for (const match of matches) {
        // æ·»åŠ thinkingæ ‡ç­¾ä¹‹å‰çš„æ–‡æœ¬ï¼ˆå¦‚æœæœ‰ï¼‰
        const beforeText = text.substring(lastEnd, match.index).trim();
        if (beforeText) {
            contentBlocks.push({
                type: "text",
                text: beforeText
            });
        }

        // æ·»åŠ thinkingå†…å®¹
        const thinkingText = match[1].trim();
        if (thinkingText) {
            contentBlocks.push({
                type: "thinking",
                thinking: thinkingText
            });
        }

        lastEnd = match.index + match[0].length;
    }

    // æ·»åŠ æœ€åä¸€ä¸ªthinkingæ ‡ç­¾ä¹‹åçš„æ–‡æœ¬ï¼ˆå¦‚æœæœ‰ï¼‰
    const afterText = text.substring(lastEnd).trim();
    if (afterText) {
        contentBlocks.push({
            type: "text",
            text: afterText
        });
    }

    // å¦‚æœæ²¡æœ‰æ‰¾åˆ°thinkingæ ‡ç­¾ï¼Œè¿”å›åŸæ–‡æœ¬
    if (contentBlocks.length === 0) {
        return text;
    }

    // å¦‚æœåªæœ‰ä¸€ä¸ªæ–‡æœ¬å—ï¼Œè¿”å›å­—ç¬¦ä¸²
    if (contentBlocks.length === 1 && contentBlocks[0].type === "text") {
        return contentBlocks[0].text;
    }

    return contentBlocks;
}

/**
 * Converts an OpenAI chat completion stream chunk to a Claude API messages stream chunk.
 * @param {Object} openaiChunk - The OpenAI API chat completion stream chunk object.
 * @param {string} [model] - Optional model name to include in the response.
 * @returns {Object} The formatted Claude API messages stream chunk.
 */
export function toClaudeStreamChunkFromOpenAI(openaiChunk, model) {
    if (!openaiChunk) {
        return null;
    }

    // å·¥å…·è°ƒç”¨
    if ( Array.isArray(openaiChunk)) {
        const toolCall = openaiChunk[0]; // å‡è®¾æ¯æ¬¡åªå¤„ç†ä¸€ä¸ªå·¥å…·è°ƒç”¨
        if (toolCall) {
            if (toolCall.function && toolCall.function.name) {
                const toolUseBlock = {
                    type: "tool_use",
                    id: toolCall.id || `call_${toolCall.function.name}_${Date.now()}`,
                    name: toolCall.function.name,
                    input: toolCall.function.arguments ? JSON.parse(toolCall.function.arguments) : {}
                };
                return { type: "content_block_start", index: 1, content_block: toolUseBlock };
            }
        }
    }

    // æ–‡æœ¬å†…å®¹
    if (typeof openaiChunk === 'string') {
        return {
            type: "content_block_delta",
            index: 0,
            delta: {
                type: "text_delta",
                text: openaiChunk
            }
        };
    }
    return null;
}

function buildGenerationConfig({ temperature, max_tokens, top_p, stop }) {
    const config = {};
    config.temperature = checkAndAssignOrDefault(temperature, DEFAULT_TEMPERATURE);
    config.maxOutputTokens = checkAndAssignOrDefault(max_tokens, DEFAULT_GEMINI_MAX_TOKENS);
    config.topP = checkAndAssignOrDefault(top_p, DEFAULT_TOP_P);
    if (stop !== undefined) config.stopSequences = Array.isArray(stop) ? stop : [stop];
    return config;
}

/**
 * Converts an OpenAI chat completion request body to a Claude API request body.
 * Handles system instructions, tool calls, and multimodal content.
 * @param {Object} openaiRequest - The request body from the OpenAI API.
 * @returns {Object} The formatted request body for the Claude API.
 */
export function toClaudeRequestFromOpenAI(openaiRequest) {
    const messages = openaiRequest.messages || [];
    const { systemInstruction, nonSystemMessages } = extractAndProcessSystemMessages(messages);

    const claudeMessages = [];

    for (const message of nonSystemMessages) {
        const role = message.role === 'assistant' ? 'assistant' : 'user';
        let content = [];

        if (message.role === 'tool') {
            // Claude expects tool_result to be in a 'user' message
            // The content of a tool message is a single tool_result block
            content.push({
                type: 'tool_result',
                tool_use_id: message.tool_call_id, // Use tool_call_id from OpenAI tool message
                content: safeParseJSON(message.content) // Parse content as JSON if possible
            });
            claudeMessages.push({ role: 'user', content: content });
        } else if (message.role === 'assistant' && message.tool_calls?.length) {
            // Assistant message with tool calls - properly format as tool_use blocks
            // Claude expects tool_use to be in an 'assistant' message
            const toolUseBlocks = message.tool_calls.map(tc => ({
                type: 'tool_use',
                id: tc.id,
                name: tc.function.name,
                input: safeParseJSON(tc.function.arguments)
            }));
            claudeMessages.push({ role: 'assistant', content: toolUseBlocks });
        } else {
            // Regular user or assistant message (text and multimodal)
            if (typeof message.content === 'string') {
                if (message.content) {
                    content.push({ type: 'text', text: message.content });
                }
            } else if (Array.isArray(message.content)) {
                message.content.forEach(item => {
                    if (!item) return;
                    switch (item.type) {
                        case 'text':
                            if (item.text) {
                                content.push({ type: 'text', text: item.text });
                            }
                            break;
                        case 'image_url':
                            if (item.image_url) {
                                const imageUrl = typeof item.image_url === 'string'
                                    ? item.image_url
                                    : item.image_url.url;
                                if (imageUrl.startsWith('data:')) {
                                    const [header, data] = imageUrl.split(',');
                                    const mediaType = header.match(/data:([^;]+)/)?.[1] || 'image/jpeg';
                                    content.push({
                                        type: 'image',
                                        source: {
                                            type: 'base64',
                                            media_type: mediaType,
                                            data: data
                                        }
                                    });
                                } else {
                                    // Claude requires base64 for images, so for URLs, we'll represent as text
                                    content.push({ type: 'text', text: `[Image: ${imageUrl}]` });
                                }
                            }
                            break;
                        case 'audio':
                            // Handle audio content as text placeholder
                            if (item.audio_url) {
                                const audioUrl = typeof item.audio_url === 'string'
                                    ? item.audio_url
                                    : item.audio_url.url;
                                content.push({ type: 'text', text: `[Audio: ${audioUrl}]` });
                            }
                            break;
                    }
                });
            }
            // Only add message if content is not empty
            if (content.length > 0) {
                claudeMessages.push({ role: role, content: content });
            }
        }
    }

    const claudeRequest = {
        model: openaiRequest.model || 'claude-3-opus-20240229',
        messages: claudeMessages,
        max_tokens: checkAndAssignOrDefault(openaiRequest.max_tokens, DEFAULT_MAX_TOKENS),
        temperature: checkAndAssignOrDefault(openaiRequest.temperature, DEFAULT_TEMPERATURE),
        top_p: checkAndAssignOrDefault(openaiRequest.top_p, DEFAULT_TOP_P),
    };

    if (systemInstruction) {
        claudeRequest.system = extractTextFromMessageContent(systemInstruction.parts[0].text);
    }

    if (openaiRequest.tools?.length) {
        claudeRequest.tools = openaiRequest.tools.map(t => ({
            name: t.function.name,
            description: t.function.description || '',
            input_schema: t.function.parameters || { type: 'object', properties: {} }
        }));
        claudeRequest.tool_choice = buildClaudeToolChoice(openaiRequest.tool_choice);
    }

    return claudeRequest;
}

function buildClaudeToolChoice(toolChoice) {
    if (typeof toolChoice === 'string') {
        const mapping = { auto: 'auto', none: 'none', required: 'any' };
        return { type: mapping[toolChoice] };
    }
    if (typeof toolChoice === 'object' && toolChoice.function) {
        return { type: 'tool', name: toolChoice.function.name };
    }
    return undefined;
}

/**
 * Extracts and combines all 'system' role messages into a single system instruction.
 * Filters out system messages and returns the remaining non-system messages.
 * @param {Array<Object>} messages - Array of message objects from OpenAI request.
 * @returns {{systemInstruction: Object|null, nonSystemMessages: Array<Object>}}
 *          An object containing the system instruction and an array of non-system messages.
 */
export function extractAndProcessSystemMessages(messages) {
    const systemContents = [];
    const nonSystemMessages = [];

    for (const message of messages) {
        if (message.role === 'system') {
            systemContents.push(extractTextFromMessageContent(message.content));
        } else {
            nonSystemMessages.push(message);
        }
    }

    let systemInstruction = null;
    if (systemContents.length > 0) {
        systemInstruction = {
            parts: [{
                text: systemContents.join('\n')
            }]
        };
    }
    return { systemInstruction, nonSystemMessages };
}

/**
 * Extracts text from various forms of message content.
 * @param {string|Array<Object>} content - The content from a message object.
 * @returns {string} The extracted text.
 */
export function extractTextFromMessageContent(content) {
    if (typeof content === 'string') {
        return content;
    }
    if (Array.isArray(content)) {
        return content
            .filter(part => part.type === 'text' && part.text)
            .map(part => part.text)
            .join('\n');
    }
    return '';
}

/**
 * Converts a Claude API request body to a Gemini API request body.
 * Handles system instructions and multimodal content.
 * @param {Object} claudeRequest - The request body from the Claude API.
 * @returns {Object} The formatted request body for the Gemini API.
 */
export function toGeminiRequestFromClaude(claudeRequest) {
    // Ensure claudeRequest is a valid object
    if (!claudeRequest || typeof claudeRequest !== 'object') {
        console.warn("Invalid claudeRequest provided to toGeminiRequestFromClaude.");
        return { contents: [] };
    }

    const geminiRequest = {
        contents: []
    };

    // Handle system instruction
    if (claudeRequest.system) {
        let incomingSystemText = null;
        if (typeof claudeRequest.system === 'string') {
            incomingSystemText = claudeRequest.system;
        } else if (typeof claudeRequest.system === 'object') {
            incomingSystemText = JSON.stringify(claudeRequest.system);
        } else if (claudeRequest.messages?.length > 0) {
            // Fallback to first user message if no system property
            const userMessage = claudeRequest.messages.find(m => m.role === 'user');
            if (userMessage) {
                if (Array.isArray(userMessage.content)) {
                    incomingSystemText = userMessage.content.map(block => block.text).join('');
                } else {
                    incomingSystemText = userMessage.content;
                }
            }
        }
        geminiRequest.systemInstruction = {
            parts: [{ text: incomingSystemText}] // Ensure system is string
        };
    }

    // Process messages
    if (Array.isArray(claudeRequest.messages)) {
        claudeRequest.messages.forEach(message => {
            // Ensure message is a valid object and has a role and content
            if (!message || typeof message !== 'object' || !message.role || !message.content) {
                console.warn("Skipping invalid message in claudeRequest.messages.");
                return;
            }

            const geminiRole = message.role === 'assistant' ? 'model' : 'user';
            const processedParts = processClaudeContentToGeminiParts(message.content);

            // If the processed parts contain a function response, it should be a 'function' role message
            // Claude's tool_result block does not contain the function name, only tool_use_id.
            // We need to infer the function name from the previous tool_use message.
            // For simplicity in this conversion, we'll assume the tool_use_id is the function name
            // or that the tool_result is always preceded by a tool_use with the correct name.
            // A more robust solution would involve tracking tool_use_ids to function names.
            const functionResponsePart = processedParts.find(part => part.functionResponse);
            if (functionResponsePart) {
                geminiRequest.contents.push({
                    role: 'function',
                    parts: [functionResponsePart]
                });
            } else if (processedParts.length > 0) { // Only push if there are actual parts
                geminiRequest.contents.push({
                    role: geminiRole,
                    parts: processedParts
                });
            }
        });
    }

    // Add generation config
    const generationConfig = {};
    generationConfig.maxOutputTokens = checkAndAssignOrDefault(claudeRequest.max_tokens, DEFAULT_GEMINI_MAX_TOKENS);
    generationConfig.temperature = checkAndAssignOrDefault(claudeRequest.temperature, DEFAULT_TEMPERATURE);
    generationConfig.topP = checkAndAssignOrDefault(claudeRequest.top_p, DEFAULT_TOP_P);
    
    if (Object.keys(generationConfig).length > 0) {
        geminiRequest.generationConfig = generationConfig;
    }

    // Handle tools
    if (Array.isArray(claudeRequest.tools)) {
        geminiRequest.tools = [{
            functionDeclarations: claudeRequest.tools.map(tool => {
                // Ensure tool is a valid object and has a name
                if (!tool || typeof tool !== 'object' || !tool.name) {
                    console.warn("Skipping invalid tool declaration in claudeRequest.tools.");
                    return null; // Return null for invalid tools, filter out later
                }

                delete tool.input_schema.$schema;
                return {
                    name: String(tool.name), // Ensure name is string
                    description: String(tool.description || ''), // Ensure description is string
                    parameters: tool.input_schema && typeof tool.input_schema === 'object' ? tool.input_schema : { type: 'object', properties: {} }
                };
            }).filter(Boolean) // Filter out any nulls from invalid tool declarations
        }];
        // If no valid functionDeclarations, remove the tools array
        if (geminiRequest.tools[0].functionDeclarations.length === 0) {
            delete geminiRequest.tools;
        }
    }

    // Handle tool_choice
    if (claudeRequest.tool_choice) {
        geminiRequest.toolConfig = buildGeminiToolConfigFromClaude(claudeRequest.tool_choice);
    }

    return geminiRequest;
}

/**
 * Builds Gemini toolConfig from Claude tool_choice.
 * @param {Object} claudeToolChoice - The tool_choice object from Claude API.
 * @returns {Object|undefined} The formatted toolConfig for Gemini API, or undefined if invalid.
 */
function buildGeminiToolConfigFromClaude(claudeToolChoice) {
    if (!claudeToolChoice || typeof claudeToolChoice !== 'object' || !claudeToolChoice.type) {
        console.warn("Invalid claudeToolChoice provided to buildGeminiToolConfigFromClaude.");
        return undefined;
    }

    switch (claudeToolChoice.type) {
        case 'auto':
            return { functionCallingConfig: { mode: 'AUTO' } };
        case 'none':
            return { functionCallingConfig: { mode: 'NONE' } };
        case 'tool':
            if (claudeToolChoice.name && typeof claudeToolChoice.name === 'string') {
                return { functionCallingConfig: { mode: 'ANY', allowedFunctionNames: [claudeToolChoice.name] } };
            }
            console.warn("Invalid tool name in claudeToolChoice of type 'tool'.");
            return undefined;
        default:
            console.warn(`Unsupported claudeToolChoice type: ${claudeToolChoice.type}`);
            return undefined;
    }
}

/**
 * Processes Claude content to Gemini parts format with multimodal support.
 * @param {string|Array} content - Claude message content.
 * @returns {Array} Array of Gemini parts.
 */
function processClaudeContentToGeminiParts(content) {
    if (!content) return [];

    // Handle string content
    if (typeof content === 'string') {
        return [{ text: content }];
    }

    // Handle array content (multimodal)
    if (Array.isArray(content)) {
        const parts = [];

        content.forEach(block => {
            // Ensure block is a valid object and has a type
            if (!block || typeof block !== 'object' || !block.type) {
                console.warn("Skipping invalid content block in processClaudeContentToGeminiParts.");
                return;
            }

            switch (block.type) {
                case 'text':
                    if (typeof block.text === 'string') {
                        parts.push({ text: block.text });
                    } else {
                        console.warn("Invalid text content in Claude text block.");
                    }
                    break;

                case 'image':
                    if (block.source && typeof block.source === 'object' && block.source.type === 'base64' &&
                        typeof block.source.media_type === 'string' && typeof block.source.data === 'string') {
                        parts.push({
                            inlineData: {
                                mimeType: block.source.media_type,
                                data: block.source.data
                            }
                        });
                    } else {
                        console.warn("Invalid image source in Claude image block.");
                    }
                    break;

                case 'tool_use':
                    if (typeof block.name === 'string' && block.input && typeof block.input === 'object') {
                        parts.push({
                            functionCall: {
                                name: block.name,
                                args: block.input
                            }
                        });
                    } else {
                        console.warn("Invalid tool_use block in Claude content.");
                    }
                    break;

                case 'tool_result':
                    // Claude's tool_result block does not contain the function name, only tool_use_id.
                    // Gemini's functionResponse requires a function name.
                    // For now, we'll use the tool_use_id as the name, but this is a potential point of failure
                    // if the tool_use_id is not the actual function name in Gemini's context.
                    // A more robust solution would involve tracking the function name from the tool_use block.
                    if (typeof block.tool_use_id === 'string') {
                        parts.push({
                            functionResponse: {
                                name: block.tool_use_id, // This might need to be the actual function name
                                response: { content: block.content } // content can be any JSON-serializable value
                            }
                        });
                    } else {
                        console.warn("Invalid tool_result block in Claude content: missing tool_use_id.");
                    }
                    break;

                default:
                    // Handle any other content types as text if they have a text property
                    if (typeof block.text === 'string') {
                        parts.push({ text: block.text });
                    } else {
                        console.warn(`Unsupported Claude content block type: ${block.type}. Skipping.`);
                    }
            }
        });

        return parts;
    }

    return [];
}

/**
 * Converts a Gemini API response to a Claude API messages response.
 * @param {Object} geminiResponse - The Gemini API response object.
 * @param {string} model - The model name to include in the response.
 * @returns {Object} The formatted Claude API messages response.
 */
export function toClaudeChatCompletionFromGemini(geminiResponse, model) {
    // Handle cases where geminiResponse or candidates are missing or empty
    if (!geminiResponse || !geminiResponse.candidates || geminiResponse.candidates.length === 0) {
        return {
            id: `msg_${uuidv4()}`,
            type: "message",
            role: "assistant",
            content: [], // Empty content for no candidates
            model: model,
            stop_reason: "end_turn", // Default stop reason
            stop_sequence: null,
            usage: {
                input_tokens: geminiResponse?.usageMetadata?.promptTokenCount || 0,
                output_tokens: geminiResponse?.usageMetadata?.candidatesTokenCount || 0
            }
        };
    }

    const candidate = geminiResponse.candidates[0];
    const content = processGeminiResponseToClaudeContent(geminiResponse);
    const finishReason = candidate.finishReason;
    let stopReason = "end_turn"; // Default stop reason

    if (finishReason) {
        switch (finishReason) {
            case 'STOP':
                stopReason = 'end_turn';
                break;
            case 'MAX_TOKENS':
                stopReason = 'max_tokens';
                break;
            case 'SAFETY':
                stopReason = 'safety';
                break;
            case 'RECITATION':
                stopReason = 'recitation';
                break;
            case 'OTHER':
                stopReason = 'other';
                break;
            default:
                stopReason = 'end_turn';
        }
    }

    return {
        id: `msg_${uuidv4()}`,
        type: "message",
        role: "assistant",
        content: content,
        model: model,
        stop_reason: stopReason,
        stop_sequence: null,
        usage: {
            input_tokens: geminiResponse.usageMetadata?.promptTokenCount || 0,
            output_tokens: geminiResponse.usageMetadata?.candidatesTokenCount || 0
        }
    };
}

/**
 * Processes Gemini response content to Claude format.
 * @param {Object} geminiResponse - The Gemini API response.
 * @returns {Array} Array of Claude content blocks.
 */
function processGeminiResponseToClaudeContent(geminiResponse) {
    if (!geminiResponse || !geminiResponse.candidates || geminiResponse.candidates.length === 0) return [];

    const content = [];

    geminiResponse.candidates.forEach(candidate => {
        if (candidate.content && candidate.content.parts) {
            candidate.content.parts.forEach(part => {
                if (part.text) {
                    content.push({
                        type: 'text',
                        text: part.text
                    });
                } else if (part.inlineData) {
                    content.push({
                        type: 'image',
                        source: {
                            type: 'base64',
                            media_type: part.inlineData.mimeType,
                            data: part.inlineData.data
                        }
                    });
                } else if (part.functionCall) {
                    // Convert Gemini functionCall to Claude tool_use
                    content.push({
                        type: 'tool_use',
                        id: uuidv4(), // Generate a new ID for the tool use
                        name: part.functionCall.name,
                        input: part.functionCall.args || {}
                    });
                }
            });
        }
    });

    return content;
}

/**
 * Converts a Gemini API stream chunk to a Claude API messages stream chunk.
 * @param {Object} geminiChunk - The Gemini API stream chunk object.
 * @param {string} [model] - Optional model name to include in the response.
 * @returns {Object} The formatted Claude API messages stream chunk.
 */
export function toClaudeStreamChunkFromGemini(geminiChunk, model) {
    if (!geminiChunk) {
        return null;
    }

    if (typeof geminiChunk === 'string') {
        return {
            type: "content_block_delta",
            index: 0,
            delta: {
                type: "text_delta",
                text: geminiChunk
            }
        };
    }

    return null;
}
